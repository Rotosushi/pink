pink started from a simple thought.

it started when i was watching a talk on functional
programming, as that was the one discipline that my
college education pretty much avoided. not for any
discriminatory reason, but for the reason that i
got my degree in embedded systems, functional languages
usually aren't used there, because they tend to require
a large runtime, and more specifically they require (?) a
garbage collector. now, i was watching a talk, i don't remember
the particular one, but the speaker was good, and he gives a lot of talks,
i've seen him plenty. i even think ive seen multiple locations
recordings of the same talk he gave. TODO: Cite this speech i saw

anyways, the point of the talk the speaker was giving was to point out the isomorphism
between the lambda calculus, and deductive reasoning. namely
the Curry-Howard correspondence. Which essentially points out that
the action of substitution, which lambda's encapsulate, is equivalent
to a single step of deductive reasoning. and because we can use lambdas
themselves to describe computation itself fully, the lambda calculus
can be used to describe deductive reasoning also.

this idea took some time to fully settle, but it has given me
so much to think about, and has legit improved my methods of thinking.
so here i share what my thoughts on this subject are, for
whatever they are worth.

additionally, and more personally, I specifically wanted to know if
the definition of the lambda calculus itself presents problems such
that the use of a garbage collector becomes required, or if the
problem was just hard, and a garbage collector (in it's first and
more primitive forms) is a simple solution to this complex problem.
(which is good, and i'm not knocking the existence of garbage collectors.
 but the introduction of the storage overhead and timing delays it makes
 programming at a very small scale very hard. now, a linked list is not
 'hard', nor does it require an obscene amount of overhead, using a very
 primitive memory allocation structure like C-has, with perhaps more attention
 to memory allocation security flaws than original C allocators. we
 can have an amount of dynamic memory usage at a small scale.

 the real key to this, i believe is if we can provide a switch, to
 turn off this dynamic memory, and any language feature that used it,
 in the event that any amount of dynamic memory usage is bad. but
 the normal action of calling a procedure need not incur any dynamic
 memory overhead either. (it only needs the stack, just like C)
 it is only the 'new' case, new from the C perspective,
 of having a representation of a partly applied procedure. that is, the
 common functional language cases of storing a procedure as a value,
 that is, not applying any arguments to a function and storing it, and
 applying less than the formally required arguments of the procedure.
 since functional languages are generally interpreted, they can
 conceive of a function in a dynamic sense, however this is not
 the case when we are talking about a sequence of instructions living
 at some constant address in memory.)

(hint: its storing procedures as values, and defining a semantic
to conceive of Currying against static procedures sitting in memory.
hint to this hint: these are the same problem :)

(branching thought, if we disallow procedures themselves (i know right O.O),
and thus only have sequence, conditionals, and iteration, we can provide a
language to computers that only have static memory and registers.
(well, we also need to disallow assignment in that case, it's basically
decls, constant calculations, and probably the ability to fill a register
in the CPU with input from some PINs, like reading a DataBus and computing
against that value which means some conception of I/O at the Word level.))

if there is some essence of the lambda calculus that makes garbage
collection a necessary component of a language defining computation
around this abstraction, then truly there is no way for an embedded
system to utilize this abstraction as it's core conception of abstraction.
it would be simply too big an idea to fit on such small computers, aww :(
and we must accept that the lambda calculus is in some sense too big
an abstraction for these small tasks.
alternatively if the problems are hard but not
untenable, then why not define a small language which then exists
as a proof of concept for this line of thinking, and at the end,
i will have learned a lot, and will have a language to show for it.
even if it's a toy language i will still be happy.



what is substitution you ask?
well, in mathematics we define a
function like

f (x) = x + x


and that describes a function over the expression containing the operator '+'

what is meant by substitution, is the act of placing a
specific value of x in the place where the syntactic x once was.
i.e. substituting each x for some concrete value.

say, 2:
f (2) = 2 + 2

or 10:
f (10) = 10 + 10

if we think, this system is then going to reduce (!!)
this expression from it's more complex state, to as
simple a state as we can get it, (fully reduce the beta-form)
it would have to perform the operation on the two values
and return us the result.

we can see that this function describes an action over
numbers. not just anything. now, as in real life and programming languages
sometimes we have actions which are valid on some things, but
not on others, usually for reasons.
(think of a time you couldn't pick something up because
it was too heavy, or a time where you missed the bus because you
showed up to the stop too late.)

now, this is related to, but distinct from the kind of problem
that is the very common while developing in programming languages.
what is meant here, is the idea of placing a type which is not
expected into the function.
something like:

f("hello") = "hello" + "hello"

now, a string is not an integer. obviously. they have to
have separate representations, they represent different sets
of values. and they communicate different information.
however, in some programming languages, the '+' symbol
is overloaded with another meaning. when a value of a different
kind is passed in, that kind may or may not be valid to
substitute within the body of the function.
if we say, only what i have defined above is what exists, then
any existing system would have no choice but to reject a bit of
code that tried to pass in some text to a procedure which expected
an integer value. this is what is known a strong/strict/static typing.
essentially, only one Type of thing can live in any single
place within the computer. the thing as that place has one
type forever and ever. now this is good in the small scale as
it allows us to say/to conceive of "This Place has this Type"
which is necessary to have any conception of difference, when
all we have is binary. so, instead of delegating the task entirely
to the programmer, which is the central conceit of providing a
base set of actions which may then be composed. (and thus, what the CPU,
and also what LLVM itself does. (it is also what C/C++ does, and
what HTML does.)). we cannot presuppose any particular composition.
that's why the construction of an assembly instruction is simply
add/sub/mul/div/operate upon the values in these two locations
and store the result in a third known location. or Jump to somewhere
if this value equals this other value. or, store this value here.
you can freely perform any operation on any address (subject to the restrictions
specified in the devices Data Sheet.) and thus the language has no
conception of "type" itself, the act of 'typing' any particular
value or memory location is thus delegated entirely to the programmer.
a programming language is thus centrally about it's conception of Type,
and it's management of the composition of Types and Operations, so that
the programmer may type 3 + 3, and doesn't necessarily have to know
that those two values live in registers persay. the lambda calculus is
a 'complete' conception of functions, and only coherent if it has a static
conception of types. (subject to extensions describing more complex types.)

the thing about Types is that they are entirely abstract, and since we
are adhering to strict typing; entirely constant and knowable at compile time.
and, as C++ has proved, with enough ingenuity you can get very nice abstractions
over static definitions that C provides. that don't cost in memory and timing
like the garbage collector does, that many functional languages are based in.

recently there have been advances in linear type systems
https://github.com/pikatchu/LinearML
that can provide functional abstractions and even multiprocessing
capabilities with no garbage collector! :)

so, it is possible. but, every type is essentially allocated within
a c++ unique_ptr. meaning movement is always destructive to the
source. (in c++ you can get a bald ptr to the member of a unique_ptr
as then a non owning reference to the memory held.)

this idea, of what kind of thing the function is valid over, is
encapsulated by the idea of a Type. the Type of an object
within a programming language which represents integer values
that lie on the integer number line are usually called
Integers. in c we say (int)

okay okay. so the central conception of the Lambda calculus is Lambdas,
given just Lambdas, you get multiple argument fully recursive procedures
(via Currying, and a Fixed Point function, like: X, or Z, or T)
and you also get a fully realized programming language right then, if you are
willing to Church encode everything. please don't be willing to Church encode
everything, because this language will take it like
writing a hundred parameter procedure for the value 100

the Central concept of C is that Pointers are essentially Static Types.
once you have pointers, you have the base of all static types.
basically, computers only work in a general way upon elements of the
size of their machine word or less. to even think about the types of
things, as like, the Type of an Array, or the Type of a Struct. you
have to think instead of the address of that Array, or the Address
of that Struct. because an array of a hundred integers, or hell even
two integers is not going to fit within a single integer sized register.
to operate upon both element of the arrays, you -must- operate on them
one machine word at a time. to operate on the first value between two
structs means extracting that machine word sized value or less.

f := \x:Int => { x + x; ...; ...; ...; };



so thus, the correspondence in a programming language like C
happens at the level of functions, we say the above idea
with the words:

int f(int x)
{
  return x + x;
}

this is a function which acts equivalently* to the mathematical
function which we defined above.
we can say they are operationally equivalent.
this is because when we substitute the value 2
into either equation, the result value is 4.
this is true no matter which order we call either
procedure, it happens no matter when we call either
procedure in fact. when we substitute 10 for x into
either equation we get the value 20, similarly this
is always the case for either procedure, forever and
ever*.

*(we will get to the subtler points
about int's fellow programmers, do not worry.)
*(we will also get to the subtler points here,
 fellow readers, do not worry)

and then the correspondence, (well, one of them anyways)

similarly, in deductive reasoning, such as what we read
about in sir Arthur Conan Doyle's books about
Sherlock Holmes, deductive reasoning is about observing
a situation, looking and seeing the actions that were
performed, then using ones own logic, intuition, and previous
understanding together to do calculations or make constrained
guesses, such as, look some footprints below a message
written on the wall. and Sherlock says, paraphrasing,
"Well, based upon the distance between the footprint's.
thus the stride of the person making the footprints, and upon
the height at which the message is written, the person would
most likely be 'such-and-such' height."

what Sherlock is performing/computing here, is similar to (hence the correspondence)
a function, which when given two values, namely,
the stride length of a person, and the height at which the message
was written, and returns a constrained guess at the persons height
we could imagine something like

float GuessHeightFromStrideAndMessage(float stride-length, float message-height)
{
  return ActualMathLeftToReader(stride-length, message-height);
}

and, you might say, hey, you just wrote a function that calls another
function and delegates solving the problem, and i'll say, hey, welcome
to deductive reasoning.

so, a correspondence is just, two things acting in a similar way to
one another? well, yeah. like what we point out with metaphor
and simile. in a way, this is sort of the core intuition behind
mathematics. the number 1 is completely abstract, it doesn't exist
at all, i cannot touch, taste, smell, hear, or see the number one.
however, anytime I have a single thing, just the one, I have
one of something. the number one is in correspondence with my
reality that i have one of something. when i get another thing,
how many things do i have then? well, two right?
but two, is just like one, it doesn't exist. the things exist, for
sure, i can touch and manipulate the things. but the number of things,
is not something i can touch. it is an abstraction that has correspondence
with that particular aspect of my reality. (the number of things I have)

to give a more powerful example, what is this equation

F = d p / d t

If you aren't a physicist, you might not immediately recognize this
equation/function, but it is Newtons Second Law of Motion. namely
that the rate of change of momentum of a body over time is
directly proportional to the force applied, and occurs in the
same direction as the applied force.

this equation is great at the scale on which we live and breathe,
even to the extent of being able to be used to deduce/compute
the positions of the celestial bodies relative to their computable/deducable
mass and acceleration. This is what Newton did in Pricipia Mathematica,
Newtons laws explained the set of functions Kepler defined,
that describe planetary motion as well. Kepler defined his procedures from
observations he and many other astronomers had collected for hundreds of
years. Newtons laws can be used to describe everything that Kepler describes,
except Newton does it with fewer procedures that instead are composed to
compute results that one wants.

when we make observations, relative to numbers, and combine and
manipulate them we can sometimes notice that the numbers follow
some underlying pattern of movement. functions essentially describe this
underlying pattern in whatever language they are being expressed in.
for mathematics we have functions, as humans have conceived of them for
hundreds of years at this point. for computers we have functions, as we
have conceived of them for just about a hundred years now. (1920's~1930's)

however, if we go back in time we can ask, what is a computer?
and the answer would be a human being who does computations.
these humans were usually accountants, or log-book-keepers of
resources, or Scientists, or Historians, or Builders. their computations
were field specific, and relative to their local context.
the idea of constructing a narrative using pieces of information
which are interacting is also something we can approximate with
deductive reasoning. this is precisely what Newton did with his
equations. the test of their truth, was if their output was consistent
with independent observations of reality. and it was hundreds of years
of true observations which aligned with Newton before we discovered
interactions which do not follow Newtons laws. (very very fast things,
very very small things, and very very heavy things)
 however it would
be equivalent to say that the laws are not capturing the
correct information in their pattern of action such that they
maintain their alignment with observable reality.

essentially, they were built out of the fact that some computations
are carried out more often than others. and as humans repeatedly do
the computations that allows them to think about the movement of the
numbers. when we repeatedly observe the movement, we can then notice
what is happening again and again, in all the different ways that it happens
and we can observe what is the same and what is different when the same
and different things are happening. (usually we constrain between a starting
point and a goal, such as predict where the planets are going to be,
because we can notice that the planets move in a cyclical movement around
our local context, i.e. repeatedly move, but move back and forth in a
consistent cycle across years of time.)

another thing that was discovered is that without Types, the lambda
calculus is undecidable. essentially it's internal logic actually breaks
down and it becomes unusable.

okay, so now that we have an idea of what correspondence is and that
the lambda calculus corresponds with some of the fundamental rules
of deductive reasoning. (and category theory as well).
we can ask, what is Pink?
well, it comes from my question: "what precisely is the problem with
expressing the simply typed lambda calculus with C-style procedures?"
which was followed up in my head with the central example of their
correspondence, namely, surrounding operators.

as above, any functional language provides facilities to add two
integer values. and, so does any imperative language. and both styles
provide the ability to define procedures which take arguments of
integer type, and perform some specific operation upon them, and then
return their result. essentially, this fragment of the lambda
calculus

(f := \x:int => x + x)

is operationally equivalent to this fragment of c code

int f (int x)
{
  return x + x;
}

so, my question, more specifically is,
what is the issue, or issues, that prevent(s) C-style procedures
from being fully equivalent to the lambda calculus procedures?
because, they do have correspondence under certain constraints.
what are those constraints and why? why do functional languages
need garbage collectors? why do functional languages use the
heap to store their stack frames? why are most functional languages
interpreted instead of being compiled? (not that there aren't functional
languages that can be compiled, just that they are usually used in an
interpreted fashion.)

from now on, we must reckon with the bowels of the machine.
for no longer can we avoid the details of implementation.
we need to specify the details of implementation with the language,
not the other way 'round.

so, as we say above, the simply typed lambda calculus, strictly evaluated
is operationally equivalent to a c-style procedure when we constrain the
question of what types can be bound to arguments to simple, primitive
types? well, yes. the short answer, using the language of a day-to-day
programmer is that any type that has a size and whose exact type is
known at compile time is a valid argument. which is nearly everything
in a c program, int's fit this mold, as do floats, as do enumerations,
and structures, which are composed of named fields of different types
added together, have constant knowable storage and type.

but it isn't one thing, procedures themselves, why aren't procedures
able to be stored in a named field in a C program? well, because a
C procedure is stored as a sequence of physical instructions.
we do not want to duplicate that sequence of instructions for
each place that the procedure is stored, as that would explode the
size of the program itself. so when we need to store a function
 C instead provides a
different solution, the function pointer. that is a reference to the
procedure. we can get to the procedure via the reference, but the
reference is just a reference, hence it itself has constant space and
known type.
so, why isn't a reference to a procedure being held in a variable
equivalent to a procedure being held in a variable?

that is a hard question to answer from this perspective, so we are
going to shift gears and build up from an example.

what are the situations we can imagine a procedure being in
given the rules of the lambda calculus? that is' how many ways
can we imagine using a procedure given the grammar of the language?

since we are talking about the strictly evaluated simply typed lambda
calculus we have a grammar that includes, defining a procedure, applying
a procedure, using a variable, binding a name/variable to a value, and
various primitive types and operations upon those types.

so, we are talking about combining symbols into terms and evaluating them.
that is, we are talking about compilation/interpretation.

so, we have a correspondence between lambda calculus procedures
and c-style procedures right? well not quite.

f := \x:int => x + x;

does have correspondence with the c-style procedure

int f (int x) { return x + x; }

however,

in lambda calculus we can imagine a valid stream of evaluation for
the terms

f := (\x:int => \y:int => x + y); // no application, locally storing a procedure
g := (f 3); // locally storing a partially applied result.
h := g 4; // fully calling the procedure and getting a result.

that is, we can partially apply procedures in the lambda calculus,
and the language takes care of storing the memory entirely
invisibly to the programmer.

in C, a procedure call either has the required number of arguments
or it doesn't. within the lambda calculus, we can peel away layers
of a procedure quite naturally with the syntax, and store partially
applied procedures. (storing partly applied procedures in arguments
is the basis for the fixed point combinator.) more specifically, partially applied multiple
argument procedures, under the lambda calculus, technically there is
only single argument procedures, and technically we are performing
one application after another, in a sequence when we encounter the
term applying the multiple argument procedure to multiple actual
values. we want express both Currying and Partial Application, what is the solution?
well, one clue is that given the existence of currying we can instead
treat a lambda whose body is immediately another lambda as a part of
an argument list, instead of an actual beginning of the body of the
lambda which returns another lambda. this more directly makes the below
situation slightly distinct from the
above calling sequence.

f := (\x:Int => (y := 2; \z:Int => (x + y + z)));

f : Int -> (Int -> Int)

g := f 2;

g is-a [x := 2, y := 2](\z:Int => x + y + z);

g : Int -> Int

h := g 2

h := Int

h is-a Int(6)

(parenthesis added for explicit scoping, but the langauge
parses the above as above with the current grammar rules,
sans any parenthesis. (and currently we don't have the sequence operator.))

so, this is a procedure who's result is a procedure, but that procedure
doesn't participate in application in the usual way. namely, it isn't
curryed. one consequence of adopting currying as equivalent to multi
argument procedure directly, is that when we encounter a procedure as
a result of application it is a distinct syntactic situation, because
the parser treats the list as the full argument list, so while in the
syntax of the actual lambda calculus, this is legal

f := (\x:Int => (y := 2; \z:Int => (x + y + z)));

f : Int -> (Int -> Int)

g := f 2 2

where then g := 6

instead, Pink would complain that
the application provides too many arguments to a
procedure of type (Int -> (Int -> Int))
(as we have sorta defined it to take one argument
and return a procedure type as the result.)
this is because the result type is not a participant
in The normal process of Currying when we are not
interpreting the syntax more directly with the
evaluation rules of the calculus.

because the calculus actually treats
f 2 2 as equivalent to (f 2) 2 anyways,
that's what makes currying equivalent to
multiple argument procedures in the first place.

if we added a case where, if
the argument list was longer, but the result
type was itself a function type, we could
compile that application to one application of
the outer fn, and then with the rest of the
present argument list try to construct another
application of the returned procedure (we know
where it will be stored by the nature of this term
allocating space for it) and we can interpret the
rest of the argument list as arguments to the
inner procedure.
potentially allowing for a single textual application
to call more than one procedure in succession,
leading to either a result value if the correct
number of arguments was provided, or a closure
term if less than the required number of arguments
was provided. or an error in the case where the
return type is not a procedure type and an argument
still exists in the application term. this is
obviously a mistake.


because we define
a locally scoped variable which is then subsequently captured by the inner
procedure, that is being returned. in this situation, a function pointer
to the inner procedure being returned out of the procedure 'f' is not
enough information to fully describe the inner procedure, and the
usual answer, which is to pass a reference into the inner procedure as
an argument, doesn't work, because according to the calling semantics
of C, the referenced memory will be reclaimed by the system as soon as
the procedure 'f' returns. that is, it's lifetime ends.
to support the idea of closing over local data, we need some way of extending
the lifetime of the pointed to data longer than the lifetime of
the procedure that it is defined in, and we need to perhaps uniquely
define this for each 'instance' of a procedure is stored. imagine the
rather funny procedure

times := (\x:int => z := x; \y:int => y * z);

when you call times,

double := times 2

double now contains a callable object, but that callable object
is referencing data that was declared and thus stored within the
function times. so, instead of a procedure reference being enough
to represent the procedure double, instead we need a procedure
reference and a reference to a by-value copy of the binding
that was closed over by the procedure, to be passed in and subsequently
used by the inner procedure. the method of lifetime extension available
to us is the heap. and heap storage. this is the beginning of the
impetus behind the garbage collector.

this is actually the same as the idea behind partial application,
it's just a question of if the data being closed over is an explicit
binding from an argument, or if the procedure is referencing some memory
which is defined in the local/dynamic scope surrounding the definition
of the inner procedure. the bind operation is present in both cases,
it's just a matter of if the binding is going alongside the definition
of a new abstraction or not. both happen entirely before we apply the
defined procedure. we know the closed over values in both cases.
actually, both the argument and the local definition live locally
in the stack frame of the procedure in C.

in the case of a partial application, the list of partially applied
bindings is a list of the arguments which are going to be placed into
argument positions on the stack once we have a complete procedure call.
the set of closed over values is constructed at the time in which we encounter
the definition of the inner procedure textually, and is the complete set of
needed definitions by-value. we could subsequently partially apply the
procedure which closed over some values at time of definition,
which would imply the presence of both lists. and both sets of bindings need to
be bound in the local execution environment of the procedure.
now, something else to notice. every locally defined procedure is only
ever constructable via the evaluation of its defining scope. which necessitates
an application term whose result is itself the procedure literal.
each application of the outer term has a result type of a partially applied
lambda, with some associated closure values, and a partial argument list.
in the body of the procedure, references to the closure values can be
compiled to extract the value from the tuple of closure values, and
references to arguments can be handled in the same way they are handled
in c, because the full application is carried out just like a c-style application.
it's just that there is a secret optional argument which represents the
closed over values, and this is silently used by the definition of the inner
procedure, and applications of the procedure object representing the
partially applied lambda either constructs a new partially applied lambda
or it constructs a procedure application.














a name is itself a reference (in the c++ sense exactly) to that specific
peice of data which represents it. all
actions upon the name pass through to the data
that is the name, and act upon the data
that is refferent. this is the main distinction
between pointers and references.
both however are addresses.

by considering names to be 'values' by default.
we gain the ability to define a pointer value.
we can consider a value that is itself a
pointer/reference. which allows the language
to speak about such things within the text itself.

passing by value becomes the default, with the
option to pass by pointer/reference, which
opens up different semantic options to the programmer,
which allows for engineering to happen.

i suppose we could consider the opposite, by reference
the default, and a syntax that means by value. however
C goes with the former, so arbitrarily we will keep
the same semantics. and with types that are smaller
than or equal to the size of a pointer the byvalue/byreference distinction
is largely immaterial.


forcing assignment to work only on names, would force
programmers to name the things they wanted to assign to.
which would change the structure of a loop assigning to
locations within an array as 'getting' the element, and
then assigning it. whereas C's grammar explicitly allows
those two statements to happen with a single combination
of terms. C does this by distinguishing between
values which can be assigned to, lvalues, and
values which cannot be assigned to, rvalues.

however, this is an idea that has been tailor made to
exactly contour inconsistency in the grammar.

why precisely can a reference to some local data not be modified?
mostly because there is never a reason too. local data is always
allocated in the same way, and so can be accessed through constants.
the same is true of constant global names. (and globally mutable
names always introduce complexity, and so are very few in their legitimate use.)

however, there is sometimes a reason to modify a reference to dynamic memory.
we want to grow or shrink dynamic structures or simply modify which portion
of a dynamic structure we are observing.

notice how the underlying structure of both these actions are the same,
modifying an integer which contains the address of the object being
referenced. and so fundamentally we can imagine a thing to do in
both situations. but we want to allow some things, and disallow others.


-----------------------------------------------------------------------------------









// say we have a fn like
/*
  Apply (op: int -> int -> int, x: int, y: int)
  {
    op x y
  }

  we could imagine passing in procedures which have the
  type signature 'int -> int -> int', but with the
  system we have described what we have implemented
  is to have argument values within structures stored
  locally, representing the function with pre bound values.
  this record's type could be different, even though the
  presented functions type is still 'int -> int -> int'
  this means we have to pass different sized structures
  to the same procedure code, and have the inner application
  of the passed procedure object handle that ambiguity.
  luckily it is only ambiguous in the sense of exactly how
  big the closed over argument list is and this ambiguity is
  constrained between zero and the length of the formal argument
  list itself. the number of arguments needed to
  produce the distinct static types (result values or closures)
  is constant for the definition of the term.

  the only way we could define static procedures that take
  variable sized arguments is with opaque struct pointers.
  because otherwise we have to provide a concrete type
  with a concrete knowable number of arguments. (essentially we
  would be forced to collapse the ambiguity described above, choosing
  to let this function name bind a closure with zero closed over values
  only, which is essentially the behavior of bare function pointers.)

  but, the procedure itself isn't taking a variable size argument
  list. the caller of a closure isn't always sure how many arguments
  the closure is going to provide, the procedure itself always knows
  exactly how many and what type of arguments it needs to apply to any given
  closure to build either another closure or a valid full application.
  it is always the case that
  the number of arguments the closure provides (N) is (Y)
  arguments short of completing the call, where (Y) is equal
  to the number of arguments in the presented function type of the
  closure. i.e. the function type of the object that the closure
  represents. which is the type of the binding one would specify
  in the argument list.

  to give a specific example, lets use a contrived base function.

  Int baseFn(i: Int, j: Int, n: Int, m: Int)
  {
    (i * n) + (j * m)
  }

  in our language baseFn has a type

  Int -> Int -> Int -> Int -> Int

  in llvm baseFn looks like

  %baseFnArgsType = type { i64, i64, i64, i64 }*
  %baseFnRetType  = type { i64 }*

  // we assume the caller made the allocation for
  // the argument values and the return value.
  // this is what the sret and byref parameter attributes mean.
  // well, not byref technically, but we will be writing
  // application to construct the alloca locally for the call.

  void basefn(sret u8* ret, byref u8* args)
  {
    %addr-ret = getelementptr inbounds baseFnRetType ret 0 0

    %typed-args = bitcast (args to baseFnArgsType)
    // casting a pointer to another type of pointer is
    // how we could accomplish this in c too.
    %addr-i = getelementptr inbounds baseFnArgsType typed-args 0 0
    %i      = load i64 addr-i
    %addr-j = getelementptr inbounds baseFnArgsType typed-args 0 1
    %j      = load i64 addr-j
    %addr-n = getelementptr inbounds baseFnArgsType typed-args 0 2
    %n      = load i64 addr-n
    %addr-m = getelementptr inbounds baseFnArgsType typed-args 0 3
    %m      = load i64 addr-m

    %t0 = mul i64 %i %n
    %t1 = mul i64 %j %m
    %t2 = add i64 %t0 %t1
    store %t2 %addr-ret
  }

  a closure around such a procedure would be built like:

  Add := baseFn 1 1

  timesTwo := baseFn 2 0

  Sub := baseFn -1 1


  so [Add] would be a pointer to a structure layed out in pseudoIR like:
  {fnPtr     := baseFn,
   args-desc := {i64 = sizeof(clsd-args), i8* = &(clsd-args.0), i8* = &(clsd-args.1)},
   clsd-args := {i64 = 1, i64 = 1}}

   [timesTwo]
  {fnPtr     := baseFn,
   args-desc := {i64 = sizeof(clsd-args), i8* = &(clsd-args.0), i8* = &(clsd-args.1)},
   clsd-args := {i64 = 2, i64 = 0}}

   [Sub]
  {fnPtr     := baseFn,
   args-desc := {i64 = sizeof(clsd-args), i8* = &(clsd-args.0), i8* = &(clsd-args.1)},
   clsd-args := {i64 = -1, i64 = 1}}


  to the application term, and to the Typechecker
  these closure objects [Add, timesTwo, Sub] have a type of

  Int -> Int -> Int

  and so can be bound to parameters expecting such a type.


  we could support a 'locally bound procedure' by allocating a
  closure object with the signature

  {fnPtr     := theFn,
   args-desc := {i64 = 0, i8* = nullptr},
   clsd-args := {i8* = nullptr}}

   and then we could apply this just like any other procedure.

---------------------------------------------------------------------------

  so, the way that OOP handles this is with static lookup
  tables. they store a pointer to a static structure describing
  the layout of the record in the first position of the anon
  structure, then the receiving procedure can cast this
  (void* essentially) opaque pointer to a pointer to this
  description structure. then they can use this description
  structure to access the elements. we should only need the sizes
  and argument positions right? if we need the type, for a
  cast instruction, then we need some way to get it there
  statically, which the only thing i can think of is
  with a type field in the description structure, say
  each object with a type holds it's type field in the
  object itself. then we can use this field to cast each
  type.

  we could try to do something similar, if we imagine an array of ints,
  and it's length held in an i32 together a struct.
  where the length N = the number of members within the structure + 2,
  of i32 values, where within the first position we store the size of the
  structure itself, int the second we store the expected number of
  arguments for a call, and in each subsequent i32 we store the offset one has to use
  to compute the structure elements address relative to a pointer to such a structure.
  then we can know, how many elements we need to pre apply,
  as that is stored in the array, and we have the elements known
  from the application term, but we only know the function's
  address, and not it's type at runtime, so we need to build this
  lookup structure holding the expected number of arguments too.

  so the first element of a closure is it's description
  struct. the second is the function pointer, and the third is
  each held argument value.
  the description struct is one i32 and an array of i32's
  of N size. the first i32 holds the size of the array.
  the second element, we interpret as an i32* and index it
  relative to that. the size of the array just so happens to
  be the number of held arguments plus two, the first
  element of the array is the expected number of arguments of the
  function held within the function pointer. the second
  element of array is the size of the held elements structure itself.
  then each subsequent
  element of the array holds the offset to index the held elements
  structure to get to each element.

  we then have enough information to memmove
  each element, because we know it's starting position
  and the next element (only in packed structures)
  occurs immediately after the end of the first element.
  so the differece between the two offsets held within
  the third and fourth integers is the size of the first
  element, the difference between the offsets held within
  the fourth and fifth integers is the size of the second
  element, and so on, until we are talking about the last
  element, whose size is the difference between the the
  last offset and the total size. (which is an offset to the
  end of the object.)
  if we have enough information to memmove, then we can make
  all the by value copies we need to then pass addresses of local
  memory locations. which is the value that gets passed
  when we know the precise type.
  or if the elements size would fit inside a single word
  we could move them in a register.

  closure-literal (the-function-pointer, arg0, arg1, ..., argM)
  {
    // we might be able to get away with i16 as
    // an optimization. but i32 would be safer.
    {i32, {M + 2 x i32}}; // the type descriptor
    (the-function-pointer);
    {arg0; arg1; ...; argM;} // the structure of applied arguments
  }

  we place the size of the structure of applied arguments in the first
  element of the type descriptor array.
  we place the expected number of formal arguments in the second position
  of the type descriptor array.
  then we place the offset of each element in the structure into each subsequent
  slot of the array.
  then we place each argument into it's slot in the structure
  of applied arguments.

  this does in fact increase the overhead of storing a function
  as a literal. we now have to store a description of the closure
  itself for the runtime to pull each element out and into a local
  alloca, to then pass each alloca into it's call position to then
  call the function.
  for the smallest case, where we consider a closure with no held
  arguments. a minimum of three integers, plus the function
  pointer, plus nothing.
  which is not a ton of memory. it's only around four words.

  for each argument we bind we have to store it's offset, so the
  sizeof the structure increases by one integer plus the sizeof
  the argument value itself, per argument we bind.
  so, for arguments that fit in a register, we are doubling
  or more the storage required. for arguments that must be
  passed by making a local copy, we are adding more dataoverhead
  but it gets better the larger the value is. (sometimes we need
  to store a lot of references. look at the Environment class i wrote)

  so, since the closure needs to be opaque and dynamically allocated
  i don't think we can store the functions pointer directly within
  the structure. like with how Vtables require a specific Vpointer
  to get to the correct table at runtime, we need a table of
  pointers to lookup against. we have a mapping of names to pointers
  already with InternedStrings. so we could emit a mapping between
  InternedStrings to Function Pointers, such that we as compiler
  writers could emit a runtime query to this map, by extracting
  the InternedString value from the opaque Closure, then
  emitting a lookup, then we get back a function pointer whose
  type we should know. (in the same way that we know a variables
  type by looking up it's binding in the SymbolTable.)
  constructing another lambda means emitting a binding to this table
  of the lambda's internal name to it's pointer. then we can get a
  pointer with a type via the runtime lookup
*/











-----------------------------------------------------------------------------









it is my sneaking suspicion that
assignment '<-'
(or in c '=')

and procedure abstraction
  '->'

are inverses of each other in the
mathematical sense.

if x is something (T),
and (f x) is another thing (U)

then (f x) is T -> U

if x holds some T,
and y holds another T

then

x <- y

makes x equivalent to y.


so, -> models change,
and <- models the opposite, a movement towards homogeneity.

and we can combine the forms to
model a variable changing from T to U

x <- f x;


this is illegal given the current language rules
surrounding assignment. (requires T '<-' T)
I suppose we could allow
an even more destructive form of assignment iff
we could keep track of exactly the largest entity
assigned to said variable through the lifetime of
the stack frame, and allocate enough room for the
largest as the size of said binding.
(this is essentially the same rule as union/product
of types) except that we know what type is held by
which instructions occur before some other instructions.

for example

given

procedure f1 x:Int, y:Int =>
  x + y

procedure f2 x:Int =>
  x > 100

procedure main =>
  x := 5, y := 50;
  x <- f1 x y; // before: x has type Int; after: x has type Int
  x <- f2 x;   // before: x has type Int; after: x has type Bool
  x <- f1 y (y + 1); // before: x has type Bool; after: x has type Int
  x <- f2 x;   // before: x has type Int; after: x has type Bool

the problem with this is not the compiler getting confused.
it's the programmer. but, i don't know the actual cost/
benefit to both sides of this, as i have never spent any
serious time with a language that has these semantics.
(Python for sure, i forget if Lisp/Scheme allows this with set!)





----------------------------------------------------------------

typedef identifier type-expression


we allow two types to be equivalent by name or
by structure. which each primitive and composite
type being equal between two types.

we have four type composites, References, Arrays,
Records, and Unions.
references have type : [ref T]
  procedure references have type : [ref (T -> T)]
    which is a type we can compose out of the two
    composite types
    two reference types can be considered equivalent
    if their referent types are the same.
  procedures can also take references as argument
    (ref T) -> T
    or
    T -> ref T

arrays have type     : [array [size] T]
  there are two kinds of array in C,
  what I mean is the basic static array, which is
  built using a size, but has no size baked
  into it's type. this is a debatable point at
  the moment however, and i am considering it.
  two array types can be considered equivalent
  if they have the same type. if their length
  were baked into the type would that cause issues?
  if we wanted to pass them as parameters to procedures
  most procedures which work with arrays don't care
  that the array is any particular size. (we don't
  want to have a stringlength procedure for each possible
  length of array, that would mean a procedure definition
  for each integer at a minimum, which is a huge program.)
  they just care that the array has a knowable size.
  in c arrays decompose into pointers. this I think is
  by-and-large a good choice. we want to be able to talk
  about an array on the stack, vs an array on the heap.

  an array of five T's
  array[5] T     // T my-array[5];

  an array of T's whose length is implied
  array[] ref T  // T my-array[];

  an array of T's whose length is five
  ref array[5] T  // T my-array[5];

  an array of ref T's whose length is implied
  ref array[] ref T // T* my-array[];

  an array of ref T's whose length is five
  ref array[5] ref T // T** my-array;


Records have type    : [record {label-1:T1, label-2:T2, ... label-n:TN}]


Unions  have type    : [union {active-label:TA, label-1:T1, label-2:T2, ... label-n:TN}]
