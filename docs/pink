pink started from a simple thought.

it started when i was watching a talk on functional
programming, as that was the one discipline that my
college education pretty much avoided. not for any
discriminatory reason, but for the reason that i
got my degree in embedded systems, functional languages
usually aren't used there, because they tend to require
a large runtime, and more specifically they require (?) a
garbage collector. now, i was watching a talk, i don't remember
the particular one, but the speaker was good, and he gives a lot of talks,
i've seen him plenty. i even think ive seen multiple locations
recordings of the same talk he gave. (i've also picked up some about
the talk show circuit in my time watching a butt load of programming talks)

anyways, at one point in the talk, the speaker points out the isomorphism
between the lambda calculus, and deductive reasoning. namely
the Curry-Howard correspondence. Which essentially points out that
the action of substitution, which lambda's encapsulate, is equivalent
to a single step of deductive reasoning. and because we can use lambdas
themselves to describe computation itself fully, the lambda calculus
can be used to describe deductive reasoning also. (in an abstract way
of course.)

this idea took some time to fully settle, but it has given me
so much to think about, and has legit improved my methods of thinking.
so here i share what my thoughts on this subject are, for
whatever they are worth.

what is substitution you ask?
well, in mathematics we define a
function like

f (x) = x + x


and that describes a function over the statement '+'

what is meant by substitution, is the act of placing a
specific value of x in the place where the x once was.
i.e. substituting each x for some concrete value.

say, 2.

f (2) = 2 + 2

f (10) = 10 + 10

we can see that this function describes an action over
numbers. not things. now, as in real life and programming languages
sometimes we have actions which are valid on some things, but
not on others, usually for reasons.
(think of a time you couldn't pick something up because
it was too heavy, or a time where you missed the bus because you
showed up to the stop too late.)
now, this is related to, but distinct from the kind of problem
that is the very common while developing in programming languages.
what is meant here, is the idea of placing a type which is not
expected into the function.
something like:

f("hello") = "hello" + "hello"

now, a string is not an integer. obviously. they have to
have separate representations, they represent different sets
of values. and they communicate different information.
however, in some programming languages, the '+' symbol
is overloaded with another meaning. when a value of a different
kind is passed in, that kind may or may not be valid to
substitute within the body of the function.
if we say, only what i have defined above is what exists, then
any existing system would have no choice but to reject a bit of
code that tried to pass in some text to a procedure which expected
an integer value.

this idea, of what kind of thing the function is valid over, is
encapsulated by the idea of a Type. the Type of an object
within a programming language which represents integer values
that lie on the integer number line are usually called
Integers. in c we say (int)

so thus, the correspondence in a programming language like C
happens at the level of functions, we say the above idea
with the words:

int f(int x)
{
  return x + x;
}

this is a function which acts equivalently* to the mathematical
function which we defined above.
we can say they are operationally equivalent.
this is because when we substitute the value 2
into either equation, the result value is 4.
this is true no matter which order we call either
procedure, it happens no matter when we call either
procedure in fact. when we substitute 10 for x into
either equation we get the value 20, similarly this
is always the case for either procedure, forever and
ever*.

*(we will get to the subtler points
about int's fellow programmers, do not worry.)
*(we will also get to the subtler points here,
 fellow readers, do not worry)

and then the correspondence, (well, one of them anyways)

similarly, in deductive reasoning, such as what we read
about in sir Arthur Conan Doyle's books about
Sherlock Holmes, deductive reasoning is about observing
a situation, looking and seeing the actions that were
performed, then using ones own logic intuition and previous
understandings together to do calculations or make constrained
guesses, such as, look some footprints below a message
written on the wall. and Sherlock says, paraphrasing,
"Well, based upon the distance between the footprint's.
the stride of the person making the footprints, and upon
the height at which the message is written, the person would
most likely be 'such-and-such' height."

what Sherlock is describing here, is similar to (hence the correspondence)
a function, which when given two values, namely,
the stride length of a person, and the height at which the message
was written, and returns a constrained guess at the persons height
we could imagine something like

float GuessHeightFromStrideAndMessage(float stride-length, float message-height)
{
  return ActualMathLeftToReader(stride-length, message-height);
}

and, you might say, hey, you just wrote a function that calls another
function and delegates solving the problem, and i'll say, hey, welcome
to deductive reasoning.

so, a correspondence is just, two things acting in a similar way to
one another? well, yeah. like what we point out with metaphor
and simile. in a way, this is sort of the core intuition behind
mathematics. the number 1 is completely abstract, it doesn't exist
at all, i cannot touch, taste, smell, hear, or see the number one.
however, anytime I have a single thing, just the one, I have
one of something. the number one is in correspondence with my
reality that i have one of something. when i get another thing,
how many things do i have then? well, two right?
but two, is just like one, it doesn't exist. the things exist, for
sure, i can touch and manipulate the things. but the number of things,
is not something i can touch. it is an abstraction that has correspondence
with that particular aspect of my reality. (the number of things I have)

to give a more powerful example, what is this equation

F = d p / d t

If you aren't a physicist, you might not immediately recognize this
equation/function, but it is Newtons Second Law of Motion. namely
that the rate of change of momentum of a body over time is
directly proportional to the force applied, and occurs in the
same direction as the applied force.

this equation is great at the scale on which we live and breathe,
even to the extent of being able to be used to deduce/compute
the positions of the celestial bodies relative to their computable/deducable
mass and acceleration. This is what Newton did in Pricipia Mathematica,
Newtons laws explained the set of functions Kepler defined,
that describe planetary motion as well. Kepler defined his procedures from
observations he and many other astronomers had collected for hundreds of
years. Newtons laws can be used to describe everything that Kepler describes,
except Newton does it with fewer procedures that instead are composed to
compute results that one wants.

when we make observations, relative to numbers, and combine and
manipulate them we can sometimes notice that the numbers follow
some underlying pattern of movement. functions essentially describe this
underlying pattern in whatever language they are being expressed in.
for mathematics we have functions, as humans have conceived of them for
hundreds of years at this point. for computers we have functions, as we
have conceived of them for just about a hundred years now. (1920's~1930's)

however, if we go back in time we can ask, what is a computer?
and the answer would be a human being who does computations.
these humans were usually accountants, or log-book-keepers of
resources, or Scientists, or Historians, or Builders. their computations
were field specific, and relative to their local context.
the idea of constructing a narrative using pieces of information
which are interacting is also something we can approximate with
deductive reasoning. this is precisely what Newton did with his
equations. the test of their truth, was if their output was consistent
with independent observations of reality. and it was hundreds of years
of true observations which aligned with Newton before we discovered
interactions which do not follow these laws. however it would
be more precise to say that the laws are not capturing the
correct information in their pattern of action such that they
maintain their alignment with observable reality.

essentially, they were built out of the fact that some computations
are carried out more often than others. and as humans repeatedly do
the computations that allows them to think about the movement of the
numbers. when we repeatedly observe the movement, we can then notice
what is happening again and again, in all the different ways that it happens
and we can observe what is the same and what is different when the same
and different things are happening. (usually we constrain between a starting
point and a goal, such as predict where the planets are going to be,
because we can notice that the planets move in a cyclical movement around
our local context, i.e. repeatedly move, but move back and forth in a
consistent cycle across years of time.)

another thing that was discovered is that without Types, the lambda
calculus is undecidable. essentially it's internal logic actually breaks
down and it becomes unusable. (i haven't explored why yet.)

okay, so now that we have an idea of what correspondence is and that
the lambda calculus corresponds with some of the fundamental rules
of deductive reasoning. (and category theory as well).
we can ask, what is Pink?
well, it comes from my question: "what precisely is the problem with
expressing the simply typed lambda calculus with C-style procedures?"
which was followed up in my head with the central example of their
correspondence, namely, surrounding operators.

as above, any functional language provides facilities to add two
integer values. and, so does any imperative language. and both styles
provide the ability to define procedures which take arguments of
integer type, and perform some specific operation upon them, and then
return their result. essentially, this fragment of the lambda
calculus

(f := \x:int => x + x)

is operationally equivalent to this fragment of c code

int f (int x)
{
  return x + x;
}

so, my question, more specifically is,
what is the issue, or issues, that prevent(s) C-style procedures
from being fully equivalent to the lambda calculus procedures?
because, they do have correspondence under certain constraints.
what are those constraints and why? why do functional languages
need garbage collectors? why do functional languages use the
heap to store their stack frames? why are most functional languages
interpreted instead of being compiled? (not that there aren't functional
languages that can be compiled, just that they are usually used in an
interpreted fashion.)

from now on, we must reckon with the bowels of the machine.
for no longer can we avoid the details of implementation.
we need to specify the details of implementation with the language,
not the other way 'round.

so, as we say above, the simply typed lambda calculus, strictly evaluated
is operationally equivalent to a c-style procedure when we constrain the
question of what types can be bound to arguments to simple, primitive
types? well, yes. the short answer, using the language of a day-to-day
programmer is that any type that has a size and whose exact type is
known at compile time is a valid argument. which is nearly everything
in a c program, int's fit this mold, as do floats, as do enumerations,
and structures, which are composed of named fields of different types
added together, have constant knowable storage and type.
but it isn't one thing, procedures themselves, why aren't procedures
able to be stored in a named field in a C program? well, because a
C procedure is stored as a sequence of physical instructions.
we do not want to duplicate that sequence of instructions for
each place that the procedure is stored, as that would explode the
size of the program itself. however, C instead provides a
different solution, the function pointer. that is a reference to the
procedure. we can get to the procedure via the reference, but the
reference is just a reference, hence it itself has constant space and
known type.
so, why isn't a reference to a procedure being held in a variable
equivalent to a procedure being held in a variable?
that is a hard question to answer from this perspective, so we are
going to shift gears and build up from an example.

what are the situations we can imagine a procedure being in
given the rules of the lambda calculus? that is' how many ways
can we imagine using a procedure given the grammar of the language?

since we are talking about the strictly evaluated simply typed lambda
calculus we have a grammar that includes, defining a procedure, applying
a procedure, using a variable, binding a name/variable to a value, and
various primitive types and operations upon those types.

so, we are talking about combining symbols into terms and evaluating them.
that is, we are talking about compilation/interpretation.

so, we have a correspondence between lambda calculus procedures
and c-style procedures right? well not quite.

f := \x:int => x + x;

does have correspondence with the c-style procedure

int f (int x) { return x + x; }

however,

in lambda calculus we can imagine a valid stream of evaluation for
the terms

f := (\x:int => \y:int => x + y);
g := (f 3);
g 4;

that is, we can partially apply procedures in the lambda calculus,
and the langauge takes care of storing the memory entirely
invisibly to the programmer.
in C, a procedure call either has the required number of arguments
or it doesn't. within the lambda calculus, we can peel away layers
of a procedure quite naturally with the syntax, and store partially
applied procedures. more specifically, partially applied multiple
argument procedures, under the lambda calculus, technically there is
only single argument procedures, and technically we are performing
one application after another, in a sequence when we encounter the
term applying the multiple argument procedure to multiple actual
values. however, given the existence of currying we can instead
treat a lambda whose body is immediately another lambda as a part of
an argument list, instead of an actual beginning of the body of the
lambda. which makes the below situation slightly distinct from the
above calling sequence.

f := (\x:int => (y := 2; \z:int => (x + y + z)));

(parenthesis added for explicit scoping, but the langauge
parses the above as above with the current grammar rules,
sans any parenthesis. (and currently we don't have the sequence operator.))

so, this is a procedure who's result is a procedure, but that procedure
doesn't participate in application in the usual way. because we define
a locally scoped variable which is then subsequently captured by the inner
procedure, that is being returned. in this situation, a function pointer
to the inner procedure being returned out of the procedure 'f' is not
enough information to fully describe the inner procedure, and the
usual answer, which is to pass a reference into the inner procedure as
an argument, doesn't work, because according to the calling semantics
of C, the referenced memory will be reclaimed by the system as soon as
the procedure 'f' returns. that is, it's lifetime ends.
to support the idea of closing over local data, we need some way of extending
the lifetime of the pointed to data longer than the lifetime of
the procedure that it is defined in, and we need to perhaps uniquely
define this for each 'instance' of a procedure is stored. imagine the
rather funny procedure

times := (\x:int => z := x; \y:int => y * z);

when you call times,

double := times 2

double now contains a callable object, but that callable object
is referencing data that was declared and thus stored within the
function times. so, instead of a procedure reference being enough
to represent the procedure double, instead we need a procedure
reference and a reference to a by-value copy of the binding
that was closed over by the procedure, to be passed in and subsequently
used by the inner procedure. the method of lifetime extension available
to us is the heap. and heap storage. this is the beginning of the
impetus behind the garbage collector.

this is actually the same as the idea behind partial application,
it's just a question of if the data being closed over is an explicit
binding from an argument, or if the procedure is referencing some memory
which is defined in the local/dynamic scope surrounding the definition
of the inner procedure. the bind operation is present in both cases,
it's just a matter of if the binding is going alongside the definition
of a new abstraction or not. both happen entirely before we apply the
defined procedure. we know the closed over values in both cases.
actually, both the argument and the local definition live locally
in the stack frame of the procedure in C.

in the case of a partial application, the list of partially applied
bindings is a list of the arguments which are going to be placed into
argument positions on the stack once we have a complete procedure call.
the set of closed over values is constructed at the time in which we encounter
the definition of the inner procedure textually, and is the complete set of
needed definitions by-value. we could subsequently partially apply the
procedure which closed over some values at time of definition,
which would imply the presence of both lists. and both sets of bindings need to
be bound in the local execution environment of the procedure.
now, something else to notice. every locally defined procedure is only
ever constructable via the evaluation of its defining scope. which necessitates
an application term whose result is itself the procedure literal.
each application of the outer term has a result type of a partially applied
lambda, with some associated closure values, and a partial argument list.
in the body of the procedure, references to the closure values can be
compiled to extract the value from the tuple of closure values, and
references to arguments can be handled in the same way they are handled
in c, because the full application is carried out just like a c-style application.
it's just that there is a secret optional argument which represents the
closed over values, and this is silently used by the definition of the inner
procedure, and applications of the procedure object representing the
partially applied lambda either constructs a new partially applied lambda
or it constructs a procedure application.














a name is itself a reference to that specific
peice of data which represents it. all
actions upon the name pass through the data
that is the name, and act upon the data
that is refferent. this is the main distinction
between pointers and references.
both however are addresses.

by considering names to be values by default.
we gain the ability to define a pointer value.
then we can consider a value that is itself a
pointer/reference. which allows the language
to speak about such things within the text itself.

passing by value becomes the default, with the
option to pass by pointer/reference, which
opens up different semantic options to the programmer,
which allows for engineering to happen.

i suppose we could consider the opposite, by reference
the default, and a syntax that means by value. however
C goes with the former, so arbitrarily we will keep
the same semantics. and with types that are smaller
than or equal to the size of a pointer the distinction
is largely immaterial.











it is my sneaking suspicion that
assignment '<-'
(or in c '=')

and procedure abstraction
  '->'

are inverses of each other in the
mathematical sense.

if x is something (T),
and (f x) is another thing (U)

then (f x) is T -> U

if x holds some T,
and y holds another T

then

x <- y

makes x equivalent to y.


so, -> models change,
and <- models the opposite, a movement towards homogeneity.

and we can combine the forms to
model a variable changing from T to U

x <- f x;


this is illegal given the current language rules
surrounding assignment. (requires T '<-' T)
I suppose we could allow
an even more destructive form of assignment iff
we could keep track of exactly the largest entity
assigned to said variable through the lifetime of
the stack frame, and allocate enough room for the
largest as the size of said binding.
(this is essentially the same rule as union/product
of types) except that we know what type is held by
which instructions occur before some other instructions.

for example

given

procedure f1 x:Int, y:Int =>
  x + y

procedure f2 x:Int =>
  x > 100

procedure main =>
  x := 5, y := 50;
  x <- f1 x y; // before: x has type Int; after: x has type Int
  x <- f2 x;   // before: x has type Int; after: x has type Bool
  x <- f1 y (y + 1); // before: x has type Bool; after: x has type Int
  x <- f2 x;   // before: x has type Int; after: x has type Bool

the problem with this is not the compiler getting confused.
it's the programmer. but, i don't know the actual cost/
benefit to both sides of this, as i have never spent any
serious time with a language that has these semantics.
(Python for sure, i forget if Lisp/Scheme allows this with set!)





----------------------------------------------------------------

typedef identifier type-expression


we allow two types to be equivalent by name or
by structure. which each primitive and composite
type being equal between two types.

we have four type composites, References, Arrays,
Records, and Unions.
references have type : [ref T]
  procedure references have type : [ref (T -> T)]
    which is a type we can compose out of the two
    composite types
    two reference types can be considered equivalent
    if their referent types are the same.
  procedures can also take references as argument
    (ref T) -> T
    or
    T -> ref T

arrays have type     : [array [size] T]
  there are two kinds of array in C,
  what I mean is the basic static array, which is
  built using a size, but has no size baked
  into it's type. this is a debatable point at
  the moment however, and i am considering it.
  two array types can be considered equivalent
  if they have the same type. if their length
  were baked into the type would that cause issues?
  if we wanted to pass them as parameters to procedures
  most procedures which work with arrays don't care
  that the array is any particular size. (we don't
  want to have a stringlength procedure for each possible
  length of array, that would mean a procedure definition
  for each integer at a minimum, which is a huge program.)
  they just care that the array has a knowable size.
  in c arrays decompose into pointers. this I think is
  by-and-large a good choice. we want to be able to talk
  about an array on the stack, vs an array on the heap.

  an array of five T's
  array[5] T     // T my-array[5];

  an array of T's whose length is implied
  array[] ref T  // T my-array[];

  an array of T's whose length is five
  ref array[5] T  // T my-array[5];

  an array of ref T's whose length is implied
  ref array[] ref T // T* my-array[];

  an array of ref T's whose length is five
  ref array[5] ref T // T** my-array;


Records have type    : [record {label-1:T1, label-2:T2, ... label-n:TN}]


Unions  have type    : [union {active-label:TA, label-1:T1, label-2:T2, ... label-n:TN}]
