within the context of interpretation,
i believe with tree style evaluation
we must accept the use of a garbage
collector. the use of pointers to simulate
assignment introduces so much subtlety between
the graph of terms being used to describe themselves
that we need something like a general garbage collection
system to count the number of references to each object
and only free when no part of the system has a reference
to said object. (and to me, because it's interpretation
and not compilation, that is acceptable. as embedded systems
are distinctly compiled programs, specifically to avoid the
use of a large runtime. however, with that said, it does
make sense in some contexts to have an interpreter running
within your program.)
maybe if we JIT via LLVM
we can avoid garbage collection and
instead run atop what we -would- have
compiled.


this is different from the context of
compiling the input text to a linkable
library, or an executable object.




// if the term is a constant object, then this is fine as
// each branch of the tree has a reference to the same constant
// all this means is we have saved on memory, as all constant
// operations on constant objects are kosher.
// when we are talking about a mutable object, then we have
// two cases to consider, stack allocated or heap allocated.
// (sorry, in the compiled sense, in the interpreted sense
//  all objects are heap allocated, that's just the nature
//  of the beast.)
// (okay, i am starting to see why a reference counted garbage
//  collector is so used within interpreters now. it is a really
//  clean solution to the very messy problem of actually reclaiming
//  memory properly within this tree rewriting system.
//  the basic gist of this garbage collector is;
//  if a mutable object is referenced by some term bound within
//  the environment, that object is still in use, and is valid
//  memory in a valid state. if we find that all terms which
//  contain references to that memory are deleted, then we
//  know for sure that memory is free to be reclaimed by the
//  system.
//
//  however, most (all?) garbage collectors introduce
//  a scaling amount of delay to a running program, introduced
//  as more and more memory is allocated and deallocated.
//
//  and so, here i sit, thinking about shared pointers as the solution
//  here. and truly, with constants, again we are assured of their
//  lifetimes a-priori of the program running. we can hide them
//  (or references to them) where necessary and not worry too much.
//  dynamic memory allocated by the programmer, that is the true culprit.
//  (aside: mutable local variables? i don't think we can have them when
//   we enter into the domain of coroutines. (at least, passing references
//   to them (when we generate their address as the stack is live, we
//   generate a stack address, if another coroutine references that
//   memory while the first stack is not active, then it is accessing
//   whatever memory is currently allocated on the stack while a
//   different thread of execution is living on the stack, hence
//   undefined behavior.)))
//  so for a programmer to request mutable memory, that they can pass
//  around the programmer in a knowable manner, that memory -has- to
//  be heap allocated.
//)
//  this is a separate problem from accessing memory mapped devices.
//  that is the case of interpreting a specific segment of memory as
//  a particular type. (in C the common strategies are either an integer-type
//  if the device is the size of a single word. or a defined structure.
//  usually a combination of a struct union with different fields being mapped
//  to specific bit's within the structure. this is why C has bit-fields in
//  structures, and that is a nesessary feature for defining an interface over
//  a memory mapped device. in this case what we need is kindof exactly what
//  C provides with it's loose interpretation of types. we need to say,
//  interpret this segment of memory as if it were a valid object of this
//  type. and then the programmer can specify reads and writes to specific
//  fields. and the mapping of enums to integers is also helpful here in
//  that commonly hardware device flags are handled by setting specific
//  subfields of their device to one or the other pattern.
//  in a UART device one field might hold the direction of the device,
//  it could be a single bit, with one meaning receiver, and zero meaning
//  transmitter. or it could be another field which is two bits with
//  zero zero meaning no stop bits, with zero one meaning one stop bit
//  with one zero meaning two stop bits, and one one being reserved with
//  no meaning. this means that a memory mapped device can be described
//  fully in terms of a singleton type, plus a set of access and update
//  procedures, with the control header being able to be manipulated
//  at a higher level of abstraction. i.e. the programmer can specify
//  operations with the mnemonics provided by the enum instead of
//  magic numbers. and segments of the program accessing the module
//  encapsulating the driver are fully shielded from the fact that
//  the device is memory mapped.)




-------------------------------------------------------------

we need to think more about closures!

local lambda definitions, which take some of their defining
scope out with them.

we have global scope, let's denote it explicitly like

// before global initialization
{
  // global scope.
}
// after global cleanup

we can imagine anonymous 'inner' scopes

// before global initialization
{
// global scope. (0)
  {
    // inner scope (1)
  }
// global scope. (0)
  {
    // inner scope (1)
    {
      // and we can imagine nesting scopes, inside of other scopes.
      // inner scope(2)
    }
    inner scope(1)
  }
// global scope. (0)
}
// after global cleanup




if we encounter declarations within these scopes,
their lifetimes are encapsulated within the bounds
of the scoping symbols.

  {
    a : Int = 0;

    {
      b : Text = "Lorem Ipsum";
    }

    c : Float = 1.4;
  }

  to simplify implementation of this scope, we
  can say, lexically declarations can appear anywhere
  within the scope, however, all of the memory needed
  for the lifetime of the scope will be allocated
  upon entering the scope. visibility of the symbols
  will still follow the lexical ordering however, as will
  any non-constant initialization statements the programmer explicitly
  writes, the symbol b, will not be visible to the syntax of
  the program until after it's declaration, however in the
  assembly instructions representing the procedure we allocate
  enough memory to hold all three bindings, (and since the
  declarations are initialized with constants, it doesn't
  matter when exactly we initialize them. (were it to be a
  procedure call, we want that call to happen at that point
  in the text of the program. to preserve the sense that
  you are literally describing a sequence of instructions.))

functions fundamentally break this notion.
especially if one can return a procedure.

f := \ z:Nil =>
{
  a : Int = 0;
  g := \ b:String =>
  {
    b + ToString(a);
  }
}

this is a simple example, the outer procedure (f)
returns as it's result another procedure (g),
this inner procedure references some data that is declared
within (f), and as such will not exist once (f) returns.
thus (g) must close over this data before it is lost to
the scope exiting. since we are talking about a by-value language
anyways, what if we tried a by-value approach to the closure?

when we define (g) which is a local procedure, which does not
participate in currying,
we must, upon assigning the procedure
literal to the return value location,
capture the local variables that are referenced within the
body of the procedure.
i think we can store a reference
to a heap allocation containing a by-value copy of each of
the closed over values in, essentially, a hidden first argument to the
procedure. within the assembly implementation, references
to names which are closed over are not references to memory
located upon the stack, but references to the by-value copy
held within the heap allocation. (since they are named we could
imagine a record/sum type doing a good job of expressing that
allocation.) this can be elided if the inner procedure doesn't
capture any locals, or if the procedure is not returned outside
of it's defining context. if we do not return the procedure outside
of it's defining context then it's variables lifetimes will match
the standard lifo ordering, just the same as applying a procedure
whose definition lies in another file, but which references only
-) memory allocated for it's arguments,
-) memory allocated locally within it's activation frame
-) memory allocated globally at the beginning of the program.
is a valid call, a procedure which is defined in a local
context, but whose address is returned as a value can be
composed into a valid calling sequence.

(if we are returning the result of applying a local lambda,
then by-definition the lambda will have evaluated before we
return outside of the defining context, for instance.
 if we
assign the procedure to a local name and the call it repeatedly
within the local context, but never return the procedure then
all of it's references to memory held within it's
defining context (free variables) will be alive
 for the entire lifetime of the procedure. (even across
 multiple local calls))

the contravening case is exactly because the lifetimes of the
free variables needs to be extended to the lifetime of the
bound procedure. the only way i can think of to extend the
lifetimes is to copy the data onto the heap, and then store
a reference to the free variables data in the binding for
the local name, such that when we consider an 'instance'
of the Lambda, we are talking about a LambdaLiteral, as opposed
to the Lambda itself. (grok that? just like Types, and LLVMTypes.)
thus, we could imagine a generator procedure like,

/*
  as an aside, returning a procedure as a result,
  is different than currying, in that there is a
  chance to define local names and do local work
  prior to returning the next procedure. where in
  a currying sense we must immediately provide
  another procedure to evaluate a multiple argument
  application expression. the difference between
  a procedure being used in a currying sense and in
  a value sense is that in the currying sense the
  evaluator, (and, thus the interpreter, and the
  equivalent compiled code), treats the multiple
  lambda bodies as if they were denoting the exact same
  shared body, essentially taking multiple arguments.
  but when we are returning a procedure as the result
  of evaluating the outer defining context, we are not
  expecting the evaluator compiled code to evaluate the
  procedure. until we encounter the value being used
  within an application expression.

  if we observe currying, we can evaluate a call list.
  if we observe a function as the resulting value,
    we need to observe another application term.
    essentially: (f a b) c d
                  ^^^^^  ^^^
                  first  second

    first is the application of the main body,
    which can be prefixed by any number of lambda
    terms, but once we observe some other evaluation
    we need to do, other than feeding arguments in,
    we do that other evaluation, and then return the
    lambda by value into the outer context. then,
    the outer context calls that inner lambda passing
    in the next arguments.
*/

g : String -> (Nil -> String) = \ s:String =>
{
  a : Int = RandomInt ();
  f := \ z:Nil =>
  {
    s + ToString a
  };
  return f;
};

this procedure appends a random integer value to
a string. it does this by returning a procedure
which does the appending to the two bits of data
that are bound in the defining context by different
methods, yet they are both bound in the inner procedure
as free variables. this means that the definintion of
the inner procedure will take two hidden free variables
as a parameter, maybe held within a record where each
member name is defined by the name of the free variables.

when we store the value within (f) and the return slot of
this procedure, we need to store it alongside it's free bindings.
(and if you were to be able to modify the local after the capture
process occured, the captured variable is a by-value copy, and
so you would not modify the captured value. this is the same
behavior as by-value parameters in regular C code. if you want
to modify a non-temporary value you make the temporary value
a reference, and then you can point it to a local or to
other heap memory.)
the procedure itself was defined with these free variables
visible, (it is a local definition after all). and so
it's assembly version can be generated such that references
to the free variables are implemented as getting the value
out of the heap storage. references to local names can still
be implemented as accessing the activation frame of the
procedure. references to global names can still be implemented
as they are in C. (in fact, if we really wanted to get devilish,
we know for a fact that if all uses of a locally defined procedure
occur within the defining scope, then references to free variables
could be made by quite literally reaching up into the activation
record above, because by definition, to reach the inner procedure
the outer procedure must be on the stack above. (otherwise that
would count as a use outside of it's defining context.))

(another 'solution' is to entirely disallow free variables,
and simply force the programmer into passing every binding
in as a parameter, or declaring and initializing local names.
alas, global names are free variables, by definition)
and funnily enough, global names don't cause the same
lifetime issues that inner free variables cause. because a
globals lifetime starts before main, and ends after main,
there is no physical way for any procedure which is called
by main to access the name after it is dead.

if you imagine 'static' declaration semantics with respect to the
bound term, then both the inner procedure and the outer procedure
can freely use the name, as it is allocated as a module local
essentially. this is because a static name that would be captured
this way is already independently scoped w.r.t the lifetime
of the defining context.

it is in my humble opinion that full partial coroutines can be
supported by allocating the coroutines activation record on
the stack, and by storing the address of the place where
we are jumping into the coroutine at next, in with the
binding of the coroutine, (a'la (types, type-literals) and
(lambdas, lambda-literals)) so that we can use that value as
the address to jump to, and the activation record is heap
allocated and passed in as a hidden first parameter just like
we do with the free variables. of course this means that
within the context of a coroutine, -all- name access is heap
access, and that raises questions about how we implement
constructing more than one instance of a coroutine,
such that we can use the same coroutine body (i.e. assembly
representation of iterating through an integer array) to iterate
through more than one array, by being clever with the data
we feed into the body. (we construct the iterator by passing in a
different array to the generator routine, which constructs
a CoroutineLiteral, which provides the binding to the array
into calls of the body, by providing a pointer to the data.
this pointer can be interchanged, which points the body to
different arrays. each Create, builds a new closure around the
same coroutine. to take this formal, we also need to note
that the global context needs to call Create on every coroutine
that is declared in the source text before program evaluation
begins, to set up the Activation records of each coroutine.)

it is also in my humble opinion that coroutines make the
perfect solution to the idea of providing a function like
interface to something like a UART driver.











why are there two inner scope(1)'s ?

well, because the two scopes lifetimes are disjoint
so, any name that is bound in the first scope is
not visible within the second scope. this means that
we can use the same memory allocated for the first inner scope
within the second scope, and be assured that at no further point
in the program, would we try and rely on a binding that existed
in the first inner scope, within the second inner scope. apparently
in some early languages inner scopes were supported by growing
and shrinking the activation frame of the procedure/active scope.
this is why we have frame pointers and stack pointers, when if the
activation record is constant across the lifetime of the scope
you only really -need- the stack pointer alone.

a C procedure is easily expressed in assembly.
unoptimized C is essentially what one would have
to write to gain the abstraction of a c style procedure
handrolled, just done by the compiler. (it's a tad
more conservative than a human author when unoptimized)

but what i mean, is if you want to express the idea of
a by-value monotonic procedure, taking arguments of some
type, you are going to have to copy them somewhere shared
between the caller and the callee before
you jump into the code that you want to execute.
this is done via the stack in most assembly disciplines
and that behavior is mimicked in C.

this allows individual procedure applications to be
fast. essentially, all you need to know is the address
of the procedure you are jumping into, and the set of
types which describe its argument(s) and return types.

this seems like a fantastic solution to the basic problem,
and we can build from that. the first observation that i have
is that overloading can be naturally supported by a language
with these semantics if that language chooses to allow the
choice of which procedure to apply be made by the actual types
which appear in calls to the overloaded procedure. since the
types are known at compile time, and are static at compile time,
the compiler can dispatch to the correct procedure within the
overload set at compile time. an additional note, a template-polymorphic
procedure, is essentially an overload set, where every body is
generated by the compiler according to a single template, where
the type of bindings is allowed to range over types, and again,
applications of a polymorphic procedure, given an actual concrete type,
could be dispatched to the correct template body at compile time.
a type which ranges over types cannot appear at runtime,
unless it's a union/product type, because the exact range is
specified statically. a polymorphic type, which is allowed to
range over any knowable type that exists, is not something
that exists in and of itself, only specific instances of it exist,
given specific types.
























it just so happens that by avoiding overloading and
polymorphism, you gain a lot of simplicity in implementation
for the compiler writer. now, i want to use this tool day in
day out, for low level programming, i want, as some
very intelligent programmer put it, the language to be as eggs.

i would really rather be able to have the compiler infer some of
the normal amount of dispatch that i have to write when working
with c. similarly, this isn't some new concept, its defining the
abstraction around the fact that i am talking about an integer
value, representing types, being used within switches generated
by the compiler to write dispatch routines that one normally has
to write when performing separate actions over unions anyways.

just like what i have to do by hand with an enum and an
explicit switch statement in C!

an overload set is simply a set of C procedures with the
same name and distinct types, given that we can infer the
type at compile time, due to the strict nature of the typing system
the compiler can disambiguate calls of a procedure with the same
name, by using the types applied. only if the type can take on
more than one form at runtime do we have to delay the selection until
runtime.
and, yes, this means that union/product types will carry their
active type member, to allow the type selection to happen at
runtime.





---------------------------------------------------------------
